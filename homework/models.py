import torch

import torch.nn as nn
import torch.nn.functional as F
from torchvision.models import resnet50

class CNNClassifier(torch.nn.Module):
    def __init__(self):
        super().__init__()
        """
        Your code here
        """
        self.resnet = resnet50(pretrained=True)
        self.resnet.fc = nn.Linear(in_features=2048, out_features=1024, bias=True)
        self.relu = nn.ReLU()
        self.fcn2 = nn.Linear(in_features=1024, out_features=6, bias=True)
        
        self.dropout = nn.Dropout(p=0.5)
        
        self.batch_norm = nn.BatchNorm1d(1024)
        # self.pred_score = nn.Softmax(6)

    def forward(self, x):
        """
        Your code here
        """
        x = self.resnet(x)
        x = self.batch_norm(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fcn2(x)
        # x = self.pred_score(x)
    
        return x


class FCN_ST(torch.nn.Module):
    def __init__(self):
        super().__init__()
        """
        Your code here.
        Hint: The Single-Task FCN needs to output segmentation maps at a higher resolution
        Hint: Use up-convolutions
        Hint: Use skip connections
        Hint: Use residual connections
        Hint: Always pad by kernel_size / 2, use an odd kernel_size
        """
        raise NotImplementedError('FCN_ST.__init__')

    def forward(self, x):
        """
        Your code here
        @x: torch.Tensor((B,3,H,W))
        @return: torch.Tensor((B,C,H,W)), C is the number of classes for segmentation.
        Hint: Input and output resolutions need to match, use output_padding in up-convolutions, crop the output
              if required (use z = z[:, :, :H, :W], where H and W are the height and width of a corresponding CNNClassifier
              convolution
        """
        raise NotImplementedError('FCN_ST.forward')


class FCN_MT(torch.nn.Module):
    def __init__(self):
        super().__init__()
        """
        Your code here.
        Hint: The Multi-Task FCN needs to output both segmentation and depth maps at a higher resolution
        Hint: Use up-convolutions
        Hint: Use skip connections
        Hint: Use residual connections
        Hint: Always pad by kernel_size / 2, use an odd kernel_size
        """
        raise NotImplementedError('FCN_MT.__init__')

    def forward(self, x):
        """
        Your code here
        @x: torch.Tensor((B,3,H,W))
        @return: torch.Tensor((B,C,H,W)), C is the number of classes for segmentation
        @return: torch.Tensor((B,1,H,W)), 1 is one channel for depth estimation
        Hint: Apply input normalization inside the network, to make sure it is applied in the grader
        Hint: Input and output resolutions need to match, use output_padding in up-convolutions, crop the output
              if required (use z = z[:, :, :H, :W], where H and W are the height and width of a corresponding strided
              convolution
        """
        raise NotImplementedError('FCN_MT.forward')


class SoftmaxCrossEntropyLoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(SoftmaxCrossEntropyLoss, self).__init__()

    def forward(self, inputs, targets):
        # outputs.requires_grad = True
        outputs = -1 * torch.log(torch.true_divide(torch.exp(inputs).T , torch.sum(torch.exp(inputs), dim=1)).T)
        loglik = torch.empty(targets.shape, requires_grad=False, dtype=torch.float32)
        for i, val in enumerate(targets):
            loglik[i] = outputs[i, val]
        # loglik = torch.gather(loglik.T, 0, targets[])
        # outputs = torch.sum(outputs * targets)

        # softmax = torch.exp(inputs) / torch.exp(inputs).sum(dim=1, keepdim=True)

        # # Get probabilities of true classes
        # true_class_probs = torch.gather(softmax, 1, targets.unsqueeze(1)).squeeze()

        # # Calculate negative log probabilities of true classes
        # neg_log_probs = -torch.log(true_class_probs)

        # # Calculate mean loss over batch
        # loss = neg_log_probs.mean()
        # print("This1, ", loss)
        # print("This2, ", loglik.mean())

        return loglik.mean()

model_factory = {
    'cnn': CNNClassifier,
    'fcn_st': FCN_ST,
    'fcn_mt': FCN_MT
}


def save_model(model):
    from torch import save
    from os import path
    for n, m in model_factory.items():
        if isinstance(model, m):
            return save(model.state_dict(), path.join(path.dirname(path.abspath(__file__)), '%s.th' % n))
    raise ValueError("model type '%s' not supported!" % str(type(model)))


def load_model(model):
    from torch import load
    from os import path
    r = model_factory[model]()
    r.load_state_dict(load(path.join(path.dirname(path.abspath(__file__)), '%s.th' % model), map_location='cpu'))
    return r
